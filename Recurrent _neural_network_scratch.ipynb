{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding out a recurrent neural network and LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation_functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The activation functions used are the tanh and the softmax activation functions as the tanh activation has in providing non-linearity and softmax is used at the output so that all the output values sum to 1. (Since we are dealing with a sparse indexed output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation(Z):\n",
    "     return (np.exp(Z)-np.exp(-Z))/(np.exp(Z)-np.exp(-Z)) # this is the tanh function can also be written as np.tanh(Z)\n",
    "def softmax_activation(Z):\n",
    "        e_x = np.exp(Z - np.max(Z))  # this is the code for softmax function \n",
    "        return e_x / e_x.sum(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the delta cross entropy, so what we have here is the dL/dO which is the first part of the chain rule as described in the above block. So the calculation done here is that, the predicted output is sub tracted from the particular word index in the one hot encoded format. So its like saying  where the i is the index of the right word ( which is known by having the value 1). (y_i -1 ) -> This is the derivitve if the Loss function cross-entropy and the softrmax activation combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(predicted_output,original_t_output):\n",
    "    li = []\n",
    "    grad = predicted_output\n",
    "    for i,l in enumerate(original_t_output): #check if the value in the index is 1 or not, if yes then take the same index value from the predicted_ouput list and subtract 1 from it. \n",
    "        if l == 1:\n",
    "    #grad = np.asarray(np.concatenate( grad, axis=0 ))\n",
    "            grad[i] -= 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the tanh derivitve link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation_backward(x,top_diff):\n",
    "    output = np.tanh(x)\n",
    "    return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient calculation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to take care about two things, one is that we have to take derivatives with respect to the weight and the other with respect to the previous function the particular weight multiplies with eg : let us consider yt_unactivated, it is the multiplication of output weights and the ht_activated. So from this we can say that we required a gradient for  output_weights as that have to be learnt, so that is represented by dV, now the other gradient belongs to the chain rule that was required to finf the gradient for input_weights , which is dL/dW. So that why the below function in multiplication returns two derivatives. Add the relevent image to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication_backward(weights,x,dz):\n",
    "    gradient_weight = np.array(np.dot(np.asmatrix(dz),np.transpose(np.asmatrix(x))))\n",
    "    chain_gradient = np.dot(np.transpose(weights),dz)\n",
    "    return gradient_weight,chain_gradient\n",
    "def add_backward(x1,x2,dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1,dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and weight parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell wehre we initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001    \n",
    "nepoch = 25               \n",
    "T = 4   # length of sequence\n",
    "outlook = 1\n",
    "hidden_dim = 100       \n",
    "output_dim = 80 # this is the total unique words in the vocabulary\n",
    "bptt_truncate = 2 # here instead of using vanilla back-propogation, we use a truncated back propogation, so while doing BP we look back at atmost 2 cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the weights initialization, random weight values ae generated between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_weights = np.random.uniform(0, 1, (hidden_dim,hidden_dim))\n",
    "activation_weights = np.random.uniform(0,1, (hidden_dim, hidden_dim))\n",
    "output_weights = np.random.uniform(0,1, (output_dim,hidden_dim))\n",
    "prev_memory =  np.random.uniform(0,1, (hidden_dim,outlook))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need the gradient values updating the weights, they are initialized wth the same shape sized of their respective weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dU = np.zeros(input_weights.shape)\n",
    "dV = np.zeros(output_weights.shape)\n",
    "dW = np.zeros(activation_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of taking just one hot encoded vectors for each word like the image,  I have tried out using a word embedding scenario, where I have initialized embedding vectors of 100x1 shape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(0,T):\n",
    "    x = np.random.randn(hidden_dim,1)\n",
    "    embeddings.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code I have considered an identity matrix, as in a real word secnario, an index based vocabulary approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = [2,45,10,65]\n",
    "output_string = [45,10,65,1]\n",
    "identity_matrix = np.identity(80, dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output mapper is a dictionary that index of the word and the one hot encoded vector of the word being present at that index. You print the output mapper to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mapper = {}\n",
    "for index_value in output_string :\n",
    "    output_mapper[index_value]  = identity_matrix[index_value,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{45: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 10: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 65: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 1: array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t = {}\n",
    "i=0\n",
    "for key,value in output_mapper.items():\n",
    "    output_t[i] = value\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 1: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 2: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 3: array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the RNN_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Rnn box is the one that has been used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rnn_forward(input_embedding, input_weights, activation_weights, prev_memory,output_weights):\n",
    "    forward_params = []\n",
    "    U_frd = np.dot(activation_weights,prev_memory)\n",
    "    W_frd = np.asarray(np.dot(input_weights,input_embedding))\n",
    "    sum_s = W_frd + U_frd\n",
    "    ht_activated = tanh_activation(sum_s)\n",
    "    yt_unactivated = np.asarray(np.dot(output_weights,  tanh_activation(sum_s)))\n",
    "    yt_activated = softmax_activation(yt_unactivated)\n",
    "    forward_params.append([W_frd,U_frd,sum_s,yt_unactivated])\n",
    "    return ht_activated,yt_activated,forward_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_prop(T, embeddings ,input_weights,activation_weights,prev_memory,output_weights):\n",
    "    output_strings = []\n",
    "    memory = {}\n",
    "    prev_ht_activation = prev_memory\n",
    "    for t in range(0,T):\n",
    "        curr_activation, curr_output, params = Rnn_forward(embeddings[t], input_weights, activation_weights, prev_memory,output_weights)\n",
    "        output_strings.append(curr_output)\n",
    "        prev_ht_activation = curr_activation\n",
    "        memory[\"ht\" + str(t)] = prev_ht_activation\n",
    "        memory[\"yt\" + str(t)] = curr_output\n",
    "        memory[\"params\" + str(t)] = params\n",
    "    return output_strings, memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output_mapper,predicted_output):\n",
    "    total_loss = 0\n",
    "    layer_loss = []\n",
    "    for y,y_ in zip(output_mapper.values(),predicted_output): # this for loop calculation is for the first equation, where loss for each time-stamp is calculated\n",
    "        loss = -sum(y[i]*np.log2(y_[i]) for i in range(len(y)))\n",
    "        loss = loss/ float(len(y))\n",
    "        layer_loss.append(loss) \n",
    "    for i in range(len(layer_loss)): #this the total loss calculated for all the time-stamps considered together. \n",
    "        total_loss  = total_loss + layer_loss[i]\n",
    "    return total_loss/float(len(predicted_output))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backprop(X,input_weights,activation_weights,output_weights,ht_activated,dLo,forward_params_t,diff_s,prev_s):# inlide all the param values for all the data thats there\n",
    "    W_frd = forward_params_t[0][0] \n",
    "    U_frd = forward_params_t[0][1]\n",
    "    ht_unactivated = forward_params_t[0][2]\n",
    "    yt_unactivated = forward_params_t[0][3]\n",
    "    dV,dsv = multiplication_backward(output_weights,ht_activated,dLo)\n",
    "    ds = np.add(dsv,diff_s) # used for truncation of memory \n",
    "    dadd = tanh_activation_backward(ht_unactivated, ds)\n",
    "    dmulw,dmulu = add_backward(U_frd,W_frd,dadd)\n",
    "    dW, dprev_s = multiplication_backward(activation_weights, prev_s ,dmulw)\n",
    "    dU, dx = multiplication_backward(input_weights, X, dmulu) #input weights\n",
    "    return (dprev_s, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate is used to see how many previous layers we require to compute the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backprop(embeddings,memory,output_t,dU,dV,dW,bptt_truncate,input_weights,output_weights,activation_weights):\n",
    "    T = 4\n",
    "    # we start the backprop from the first timestamp. \n",
    "    for t in range(4):\n",
    "        prev_s_t = np.zeros((hidden_dim,1)) #required as the first timestamp does not have a previous activation or memory\n",
    "        diff_s = np.zeros((hidden_dim,1)) # this is used for the truncating purpose of restoring a previous information from the before level\n",
    "        predictions = memory[\"yt\" + str(t)]\n",
    "        ht_activated = memory[\"ht\" + str(t)]\n",
    "        forward_params_t = memory[\"params\"+ str(t)] \n",
    "        dLo = delta_cross_entropy(predictions,output_t[t]) #the loss derivative for that particular timestamp\n",
    "        dprev_s, dU_t, dW_t, dV_t = single_backprop(embeddings[t],input_weights,activation_weights,output_weights,ht_activated,dLo,forward_params_t,diff_s,prev_s_t)\n",
    "        prev_s_t = ht_activated\n",
    "        prev = t-1\n",
    "        dLo = np.zeros((output_dim,1)) #here the loss deriative is turned to 0 as we do not require it for the turncated information.\n",
    "        # the following code is for the trunated bptt and its for each time-stamp. \n",
    "        for i in range(t-1,max(-1,t-bptt_truncate),-1):\n",
    "            forward_params_t = memory[\"params\" + str(i)]\n",
    "            ht_activated = memory[\"ht\" + str(i)]\n",
    "            prev_s_i = np.zeros((hidden_dim,1)) if i == 0 else memory[\"ht\" + str(prev)]\n",
    "            dprev_s, dU_i, dW_i, dV_i = single_backprop(embeddings[t] ,input_weights,activation_weights,output_weights,ht_activated,dLo,forward_params_t,dprev_s,prev_s_i)\n",
    "            dU_t += dU_i #adding the previous gradients on lookback to the current time sequence \n",
    "            dW_t += dW_i\n",
    "        dV += dV_t \n",
    "        dU += dU_t\n",
    "        dW += dW_t\n",
    "    return (dU, dW, dV)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(learning_rate, dU,dW,dV, input_weights, activation_weights,output_weights ):\n",
    "    input_weights -= learning_rate* dU\n",
    "    activation_weights -= learning_rate * dW\n",
    "    output_weights -=learning_rate * dV\n",
    "    return input_weights,activation_weights,output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(T, embeddings,output_t,output_mapper,input_weights,activation_weights,output_weights,dU,dW,dV,prev_memory,learning_rate=0.001, nepoch=100, evaluate_loss_after=2):\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "                output_string,memory = full_forward_prop(T, embeddings ,input_weights,activation_weights,prev_memory,output_weights)\n",
    "                loss = calculate_loss(output_mapper, output_string)\n",
    "                losses.append(loss)\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after  epoch=%d: %f\" % (time,epoch, loss))\n",
    "                sys.stdout.flush()\n",
    "        dU,dW,dV = rnn_backprop(embeddings,memory,output_t,dU,dV,dW,bptt_truncate,input_weights,output_weights,activation_weights)\n",
    "        input_weights,activation_weights,output_weights= sgd_step(learning_rate,dU,dW,dV,input_weights,activation_weights,output_weights)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-05 19:59:35: Loss after  epoch=0: 0.135151\n",
      "2020-09-05 19:59:35: Loss after  epoch=2: 0.133580\n",
      "2020-09-05 19:59:35: Loss after  epoch=4: 0.130551\n",
      "2020-09-05 19:59:35: Loss after  epoch=6: 0.126094\n",
      "2020-09-05 19:59:35: Loss after  epoch=8: 0.120250\n"
     ]
    }
   ],
   "source": [
    "losses = train(T, embeddings,output_t,output_mapper,input_weights,activation_weights,output_weights,dU,dW,dV,prev_memory,learning_rate=0.0001, nepoch=10, evaluate_loss_after=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
